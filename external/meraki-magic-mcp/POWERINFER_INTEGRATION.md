# PowerInfer & TurboSparse Integration

## Overview

PowerInfer and TurboSparse support has been added to the reusable framework, providing **2-5x faster inference** on consumer hardware with no API keys required.

## What Was Added

### 1. PowerInfer Backend Support

- ‚úÖ Added `AgentBackend.POWERINFER` to the backend enum
- ‚úÖ Created `PowerInferClient` class for direct PowerInfer execution
- ‚úÖ Created `PowerInferOllamaWrapper` for using TurboSparse models via Ollama
- ‚úÖ Auto-detection of PowerInfer installation
- ‚úÖ Auto-detection of TurboSparse models

### 2. Configuration Updates

- ‚úÖ PowerInfer is now **first priority** in backend detection (fastest, no API keys)
- ‚úÖ Falls back to Ollama if PowerInfer not available
- ‚úÖ Supports both direct PowerInfer and TurboSparse models via Ollama

### 3. Performance Benefits

- **2-5x faster** inference compared to standard models
- **90-97% activation sparsity** with TurboSparse models
- **Runs on consumer hardware** (PCs, smartphones)
- **No API keys required** (local inference)

## PowerInfer Setup

### Option 1: Direct PowerInfer Installation

1. **Clone PowerInfer:**
   ```bash
   git clone https://github.com/SJTU-IPADS/PowerInfer.git
   cd PowerInfer
   mkdir build && cd build
   cmake .. -DCMAKE_BUILD_TYPE=Release
   make -j
   ```

2. **Download TurboSparse Models:**
   ```bash
   # From Hugging Face
   # TurboSparse-Mistral-7B
   # TurboSparse-Mixtral-47B
   ```

3. **Configure:**
   ```python
   config = {
       "powerinfer_path": "/path/to/powerinfer",
       "powerinfer_model_path": "/path/to/TurboSparse-Mistral-7B"
   }
   ```

### Option 2: TurboSparse Models via Ollama (Easier)

If you have TurboSparse models loaded in Ollama:

```bash
# Pull TurboSparse models into Ollama
ollama pull turbosparse-mistral-7b
ollama pull turbosparse-mixtral-47b
```

The system will automatically detect and use them!

## Usage

### Automatic Detection

The TUI and AI assistant will automatically use PowerInfer if available:

```python
from reusable.simple_ai import get_ai_assistant

# Auto-detects PowerInfer (fastest, no API keys!)
assistant = get_ai_assistant(app_name="meraki_magic_mcp")
```

### Manual Configuration

```python
from reusable.agent_framework_wrapper import AgentFrameworkWrapper, AgentBackend

# Use PowerInfer explicitly
agent = AgentFrameworkWrapper(
    backend=AgentBackend.POWERINFER,
    config={
        "powerinfer_path": "/path/to/powerinfer",  # Optional
        "powerinfer_model_path": "/path/to/model",  # Optional
        "ollama_base_url": "http://localhost:11435"  # For Ollama wrapper
    }
)
```

## Backend Priority

The system tries backends in this order:

1. **PowerInfer** (fastest, 2-5x speedup) ‚úÖ **First Priority**
2. **Ollama** (local, no API keys)
3. OpenAI (requires API key)
4. Anthropic (requires API key)
5. Others...

## Models Available

### TurboSparse Models

- **TurboSparse-Mistral-7B** - 7B parameters, optimized for speed
- **TurboSparse-Mixtral-47B** - 47B parameters, runs on 24GB DRAM

### Performance

- **Generation Speed:** 2.83√ó average speedup (2-5√ó peak)
- **Mobile Inference:** ~11.68 tokens/sec on smartphones
- **Resource Efficiency:** Minimal GPU memory, runs on consumer hardware

## Testing

### Test PowerInfer Detection

```bash
python3 -c "from reusable.config import AIConfig, AgentBackend; print('PowerInfer available:', AIConfig._check_powerinfer_available()); print('Detected backend:', AIConfig.detect_backend())"
```

### Test PowerInfer Client

```python
from reusable.powerinfer_client import PowerInferClient, PowerInferOllamaWrapper

# Direct PowerInfer
client = PowerInferClient()
print("Available:", client.is_available())

# Via Ollama
wrapper = PowerInferOllamaWrapper()
print("TurboSparse models:", wrapper.turbosparse_models)
```

## Configuration

### Environment Variables (Optional)

```bash
# PowerInfer executable path
export POWERINFER_PATH="/path/to/powerinfer"

# TurboSparse model path
export POWERINFER_MODEL_PATH="/path/to/TurboSparse-Mistral-7B"

# PowerInfer API URL (if running as service)
export POWERINFER_BASE_URL="http://localhost:8080"
```

### Config File

Edit `~/.network_observability_ai_config.json`:

```json
{
  "backend": "powerinfer",
  "backend_config": {
    "powerinfer_path": "/path/to/powerinfer",
    "powerinfer_model_path": "/path/to/TurboSparse-Mistral-7B",
    "ollama_base_url": "http://localhost:11435"
  }
}
```

## Integration with TUI

The TUI will automatically use PowerInfer if available:

```bash
python3 meraki_tui.py
```

AI commands will use PowerInfer/TurboSparse for **2-5x faster responses**:

```
audit meraki_tui.py code          # Uses PowerInfer (if available)
repair "slow function" file.py    # Uses PowerInfer (if available)
optimize mcp_client.py            # Uses PowerInfer (if available)
```

## Benefits

### ‚úÖ Speed

- **2-5√ó faster** than standard models
- **2.83√ó average speedup**
- Peak speedups of 2-5√ó

### ‚úÖ Efficiency

- **90-97% activation sparsity**
- Minimal GPU memory usage
- Runs on consumer hardware (24GB DRAM for 47B model)

### ‚úÖ No API Keys

- Local inference
- No external API calls
- Complete privacy

### ‚úÖ Mobile Support

- Runs on smartphones
- ~11.68 tokens/sec on OnePlus 12
- Previously difficult for 47B models

## Files Created/Modified

- `reusable/powerinfer_client.py` - PowerInfer client implementation
- `reusable/agent_framework_wrapper.py` - Added PowerInfer backend
- `reusable/config.py` - Added PowerInfer detection and priority
- `POWERINFER_INTEGRATION.md` - This documentation

## Resources

- **PowerInfer GitHub:** https://github.com/SJTU-IPADS/PowerInfer
- **TurboSparse Models:** Available on PowerInfer Hugging Face page
- **Paper:** TurboSparse and PowerInfer research papers

## Next Steps

1. ‚úÖ PowerInfer support integrated
2. ‚úÖ Auto-detection working
3. ‚è≥ Install PowerInfer or load TurboSparse models in Ollama
4. ‚è≥ Test with TUI for 2-5x speedup

The framework is ready to use PowerInfer when available! üöÄ
